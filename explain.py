# -*- coding: utf-8 -*-
"""
Created on 4/13/25

@author: Yifei Sun
"""

import os
import time
import threading
import concurrent.futures
from openai import OpenAI

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-cluhdroduuxirlhajacclovgxfrgbcufvjcrwdwqzfjzublc",
    base_url="https://api.siliconflow.cn/v1"
)

# =============================
# âš™ï¸ Token é™é€Ÿæ§åˆ¶å·¥å…·
# =============================

class TokenBucket:
    def __init__(self, max_tokens_per_minute):
        self.capacity = max_tokens_per_minute
        self.tokens = max_tokens_per_minute
        self.lock = threading.Lock()
        self.last_refill = time.time()

    def _refill(self):
        now = time.time()
        elapsed = now - self.last_refill
        if elapsed >= 60:
            self.tokens = self.capacity
            self.last_refill = now

    def consume(self, estimated_tokens):
        while True:
            with self.lock:
                self._refill()
                if self.tokens >= estimated_tokens:
                    self.tokens -= estimated_tokens
                    return
            time.sleep(0.5)

# åˆå§‹åŒ– Token é™é€Ÿå™¨ï¼ˆ10,000 TPMï¼‰
token_bucket = TokenBucket(max_tokens_per_minute=10000)

# =============================
# ğŸ“ Token ä¼°ç®—å‡½æ•°
# =============================

def estimate_total_tokens(text):
    return max(100, int(len(text) / 3.5)) + 1500  # ç”¨äºæ•´æ®µ

def estimate_line_tokens(text):
    return max(1, int(len(text) / 4))  # ç”¨äºæŒ‰è¡Œä¼°ç®—ï¼Œæ›´ä¿å®ˆ

# =============================
# ğŸ”§ æ‹†åˆ†å†…å®¹å‡½æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
# =============================

def split_large_content(content, max_chunk_tokens=6000):
    lines = content.splitlines()
    chunks = []
    current_chunk = []
    current_token_count = 0

    for line in lines:
        line_token_est = estimate_line_tokens(line)
        if current_token_count + line_token_est > max_chunk_tokens:
            if current_chunk:
                chunks.append("\n".join(current_chunk))
            current_chunk = [line]
            current_token_count = line_token_est
        else:
            current_chunk.append(line)
            current_token_count += line_token_est

    if current_chunk:
        chunks.append("\n".join(current_chunk))

    return chunks

# =============================
# ğŸ’¬ å•æ®µè§£é‡Šå‡½æ•°
# =============================

def explain_dat_chunk(chunk, part_idx=None):
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªç»“æ„æœ‰é™å…ƒä¸“å®¶ï¼Œæ“…é•¿è§£é‡Š MSC Nastran çš„è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼ˆ.datï¼‰ã€‚è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹æ¨¡å‹è¾“å…¥éƒ¨åˆ†çš„å«ä¹‰ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹ï¼ˆGRIDï¼‰ã€å•å…ƒï¼ˆå¦‚CQUAD4ï¼‰ã€ææ–™å±æ€§ï¼ˆMAT1ï¼‰ç­‰å†…å®¹ï¼Œè¦æ±‚ç”¨ä¸­æ–‡ç®€æ´æ¸…æ™°åœ°è¯´æ˜å…¶å»ºæ¨¡ç›®çš„å’Œå…³é”®å‚æ•°ã€‚

ä»¥ä¸‹æ˜¯è¾“å…¥æ–‡ä»¶å†…å®¹ï¼š
{chunk}
"""

    estimated_token_usage = estimate_total_tokens(prompt)
    print(f"ğŸ“ æœ¬æ®µé¢„ä¼° token æ•°é‡: {estimated_token_usage}")
    token_bucket.consume(estimated_token_usage)

    response = client.chat.completions.create(
        model="Pro/deepseek-ai/DeepSeek-V3",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=8192,
        stream=True
    )

    explanation = ""
    for chunk in response:
        if not chunk.choices:
            continue
        delta = chunk.choices[0].delta
        if hasattr(delta, "content") and delta.content:
            explanation += delta.content
            # print(delta.content, end="", flush=True)
        elif hasattr(delta, "reasoning_content") and delta.reasoning_content:
            explanation += delta.reasoning_content
            # print(delta.reasoning_content, end="", flush=True)

    if part_idx is not None:
        return f"ã€ç¬¬ {part_idx + 1} æ®µè¯´æ˜ã€‘\n{explanation.strip()}"
    return explanation.strip()

# =============================
# ğŸ” ä¸»åŠŸèƒ½æ¨¡å—
# =============================

def explain_dat_file(filepath):
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    estimated_token_usage = estimate_total_tokens(content)
    print(f"ğŸ“ é¢„ä¼° token æ•°é‡: {estimated_token_usage}")

    if estimated_token_usage > 7000:
        print(f"âš ï¸ å†…å®¹è¶…è¿‡ 7000 tokensï¼Œè¿›è¡Œé€‚åº¦åˆ†æ®µå¤„ç†ï¼š{filepath}")
        chunks = split_large_content(content)
        full_explanation = ""
        for idx, chunk in enumerate(chunks):
            print(f"\nğŸ”¹ æ­£åœ¨å¤„ç†ç¬¬ {idx + 1}/{len(chunks)} æ®µ")
            full_explanation += explain_dat_chunk(chunk, idx) + "\n"
        return full_explanation.strip()
    else:
        return explain_dat_chunk(content)

def load_processed_files(output_md):
    if not os.path.exists(output_md):
        return set()
    processed = set()
    with open(output_md, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith("| `") and "`" in line:
                filename = line.split("`")[1]
                processed.add(filename)
    return processed

write_lock = threading.Lock()

def process_single_file(folder_path, filename):
    filepath = os.path.join(folder_path, filename)
    print(f"\nğŸš€ æ­£åœ¨å¤„ç†ï¼š{filename}")
    try:
        explanation = explain_dat_file(filepath)
    except Exception as e:
        print(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {e}")
        explanation = f"å‡ºé”™ï¼š{str(e)}"

    safe_expl = explanation.replace("\n", "<br>").replace("|", "\\|")
    return f"| `{filename}` | {safe_expl} |\n"

def batch_process_and_save_md_parallel(folder_path, output_md="nastran_explanations.md", max_workers=4):
    processed_files = load_processed_files(output_md)
    print(f"ğŸ”„ å·²å¤„ç†æ–‡ä»¶æ•°é‡: {len(processed_files)}")

    all_dat_files = [f for f in os.listdir(folder_path) if f.endswith(".dat")]
    remaining_files = [f for f in all_dat_files if f not in processed_files]
    print(f"ğŸ“ æœ¬æ¬¡å¾…å¤„ç†æ–‡ä»¶æ•°é‡: {len(remaining_files)}")

    first_write = not os.path.exists(output_md) or os.stat(output_md).st_size == 0
    if first_write:
        with open(output_md, 'w', encoding='utf-8') as f:
            f.write("| æ–‡ä»¶å | æ¨¡å‹è§£é‡Š |\n")
            f.write("|--------|-----------|\n")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_file, folder_path, filename): filename
            for filename in remaining_files
        }

        with open(output_md, 'a', encoding='utf-8') as f:
            for future in concurrent.futures.as_completed(futures):
                filename = futures[future]
                try:
                    result_line = future.result()
                    with write_lock:
                        f.write(result_line)
                        f.flush()
                except Exception as exc:
                    print(f"âš ï¸ æ–‡ä»¶ {filename} å†™å…¥å¤±è´¥: {exc}")

    print(f"\nâœ… æ‰€æœ‰å‰©ä½™æ–‡ä»¶å·²å¤„ç†å¹¶ä¿å­˜åˆ°ï¼š{output_md}")

# =============================
# ğŸ§ª ä¸»ç¨‹åºå…¥å£
# =============================

if __name__ == "__main__":
    batch_process_and_save_md_parallel("./data/Nastran/demo", max_workers=3)
